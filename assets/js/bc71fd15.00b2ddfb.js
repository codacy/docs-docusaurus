"use strict";(globalThis.webpackChunkdocusaurus=globalThis.webpackChunkdocusaurus||[]).push([[3481],{28453(e,i,s){s.d(i,{R:()=>a,x:()=>r});var n=s(96540);const o={},t=n.createContext(o);function a(e){const i=n.useContext(t);return n.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),n.createElement(t.Provider,{value:i},e.children)}},33708(e,i,s){s.d(i,{A:()=>n});const n=s.p+"assets/images/repositories-with-most-ai-issues-e67b4032d7214bc4460d3f2356545039.png"},39084(e,i,s){s.r(i),s.d(i,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>n,toc:()=>l});const n=JSON.parse('{"id":"organizations/ai-risk-hub","title":"AI Risk Hub","description":"The organization\'s AI Risk Hub dashboard provides an overview of all the AI issues detected in the repositories applied to the organization\'s AI Policy standard and your organization\'s risk level based on your AI practices.","source":"@site/docs/organizations/ai-risk-hub.md","sourceDirName":"organizations","slug":"/organizations/ai-risk-hub","permalink":"/docs-docusaurus/organizations/ai-risk-hub","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"AI Risk Hub","description":"The organization\'s AI Risk Hub dashboard provides an overview of all the AI issues detected in the repositories applied to the organization\'s AI Policy standard and your organization\'s risk level based on your AI practices."},"sidebar":"documentationSidebar","previous":{"title":"Issues metrics","permalink":"/docs-docusaurus/organizations/issues-metrics"},"next":{"title":"Using gate policies","permalink":"/docs-docusaurus/organizations/using-gate-policies"}}');var o=s(74848),t=s(28453);const a={title:"AI Risk Hub",description:"The organization's AI Risk Hub dashboard provides an overview of all the AI issues detected in the repositories applied to the organization's AI Policy standard and your organization's risk level based on your AI practices."},r=void 0,c={},l=[{value:"AI Policy Compliance",id:"ai-policy-compliance",level:2},{value:"Unapproved model calls",id:"unapproved-model-calls",level:3},{value:"AI Safety",id:"ai-safety",level:3},{value:"Hardcoded Secrets",id:"hardcoded-secrets",level:3},{value:"Vulnerabilities (Insecure dependencies / SCA)",id:"vulnerabilities-insecure-dependencies--sca",level:3},{value:"Repositories with most AI issues",id:"repositories-with-most-ai-issues",level:2},{value:"Risk Level",id:"risk-level",level:2},{value:"AI Risk Checklist",id:"ai-risk-checklist",level:2}];function d(e){const i={a:"a",admonition:"admonition",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(i.p,{children:["The ",(0,o.jsx)(i.strong,{children:"AI Risk Hub"})," dashboard provides an overview of all the AI issues detected in the repositories applied to the organization's AI Policy standard and your organization's risk level based on your AI practices. Here, you can navigate through the issues detected in your repositories and filter them by severity and category. You can also filter the issues by selecting specific repositories or using ",(0,o.jsx)(i.a,{href:"/docs-docusaurus/organizations/segments",children:"the segments that you have set up"}),"."]}),"\n",(0,o.jsx)(i.admonition,{type:"caution",children:(0,o.jsx)(i.p,{children:"Currently this tab is a preview of a Business tier feature."})}),"\n",(0,o.jsx)(i.p,{children:(0,o.jsx)(i.img,{alt:"AI Risk Hub dashboard",src:s(89072).A+"",width:"2680",height:"1716"})}),"\n",(0,o.jsxs)(i.p,{children:["To access the AI Risk Hub dashboard, select an organization from the top navigation bar and click on the ",(0,o.jsx)(i.strong,{children:"AI Risk Hub"})," tab at the top of the page."]}),"\n",(0,o.jsx)(i.p,{children:"The AI Risk Hub dashboard includes the following sections to help you monitor AI risk in your organization:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"#ai-policy-compliance",children:"AI Policy Compliance"})}),"\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"#repositories-with-most-ai-issues",children:"Repositories with most AI issues"})}),"\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"#risk-level",children:"Risk Level"})}),"\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"#ai-risk-checklist",children:"AI Risk Checklist"})}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"ai-policy-compliance",children:"AI Policy Compliance"}),"\n",(0,o.jsx)(i.p,{children:"Our AI Policy is a pre-defined, curated ruleset designed to prevent risks and vulnerabilities that are inherent to AI code from entering the codebase \u2013 which can be enforced immediately across all repositories and pull request checks.\nYou can enable Codacy's AI Policy by clicking on the button on the right side of the section. This creates a coding standard that applies AI related patterns to your repositories, safeguarding them from AI risks.\nWhen the policy is enabled, you are able to view a real distribution of the AI issues found distributed by severity and AI category.\nWhen you already have the AI Policy enabled, you can see an edit button which allows you to edit the repositories that have this policy applied."}),"\n",(0,o.jsx)(i.p,{children:(0,o.jsx)(i.img,{alt:"AI Policy Compliance",src:s(76876).A+"",width:"1128",height:"252"})}),"\n",(0,o.jsx)(i.p,{children:"The AI Policy covers four groups of AI-specific risks:"}),"\n",(0,o.jsx)(i.h3,{id:"unapproved-model-calls",children:"Unapproved model calls"}),"\n",(0,o.jsx)(i.p,{children:"Ensure no disallowed models are used in production and get visibility around any compliance misuses."}),"\n",(0,o.jsx)(i.h3,{id:"ai-safety",children:"AI Safety"}),"\n",(0,o.jsx)(i.p,{children:"Ensures safety practices are enforced and applied with the use of these new technologies."}),"\n",(0,o.jsx)(i.h3,{id:"hardcoded-secrets",children:"Hardcoded Secrets"}),"\n",(0,o.jsx)(i.p,{children:"Ensures anything created or used by AI is protected from misusage."}),"\n",(0,o.jsx)(i.h3,{id:"vulnerabilities-insecure-dependencies--sca",children:"Vulnerabilities (Insecure dependencies / SCA)"}),"\n",(0,o.jsx)(i.p,{children:"Ensures protection on all fronts, by integrating vulnerability detection through your entire organization."}),"\n",(0,o.jsx)(i.h2,{id:"repositories-with-most-ai-issues",children:"Repositories with most AI issues"}),"\n",(0,o.jsx)(i.p,{children:"This list displays repositories in descending order based on the number of AI issues. Depending on the filters applied, the list will show repositories with the most AI open issues, grouped by severity or AI category."}),"\n",(0,o.jsx)(i.p,{children:(0,o.jsx)(i.img,{alt:"Repositories with most AI issues",src:s(33708).A+"",width:"592",height:"529"})}),"\n",(0,o.jsx)(i.h2,{id:"risk-level",children:"Risk Level"}),"\n",(0,o.jsxs)(i.p,{children:["This panel shows the organizational AI Risk Level based on the implementation (or lack) of a range of essential AI safeguards that can be enabled in Codacy.\nThe possible risk levels are: High, Medium, and Low, considering special control factors you can enable in Codacy.\nThese control factors are specified in the ",(0,o.jsx)(i.strong,{children:"AI Risk Checklist"}),"."]}),"\n",(0,o.jsx)(i.p,{children:(0,o.jsx)(i.img,{alt:"Risk Level",src:s(70779).A+"",width:"387",height:"191"})}),"\n",(0,o.jsx)(i.h2,{id:"ai-risk-checklist",children:"AI Risk Checklist"}),"\n",(0,o.jsx)(i.p,{children:"With most repositories today being subject to GenAI code contributions, the checklist covers essential source code controls that we recommend to enable across all projects within your organization:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"AI Policy enabled: Enable the AI Policy inside the AI Risk Hub tab."}),"\n",(0,o.jsxs)(i.li,{children:["Coverage enabled: Set up code coverage for your repositories. See how to ",(0,o.jsx)(i.a,{href:"/docs-docusaurus/coverage-reporter/",children:"upload coverage data"})," to Codacy."]}),"\n",(0,o.jsxs)(i.li,{children:["Enforced gates: Add ",(0,o.jsx)(i.a,{href:"/docs-docusaurus/repositories-configure/adjusting-quality-gates",children:"gates to your repositories"}),", and preferentially ",(0,o.jsx)(i.a,{href:"/docs-docusaurus/organizations/using-gate-policies",children:"apply repositories to gate policies"}),"."]}),"\n",(0,o.jsxs)(i.li,{children:["Protected pull requests: Protect your pull requests by ",(0,o.jsx)(i.a,{href:"../getting-started/integrating-codacy-with-your-git-workflow.md#blocking-pull-requests",children:"enforcing quality gates"}),"."]}),"\n",(0,o.jsxs)(i.li,{children:["Daily vulnerability scans: ",(0,o.jsx)(i.a,{href:"/docs-docusaurus/organizations/managing-security-and-risk#dependencies-list",children:"Enable Proactive SCA"})," to protect your repositories from dependencies vulnerabilities."]}),"\n",(0,o.jsxs)(i.li,{children:["Applications scanned: ",(0,o.jsx)(i.a,{href:"/docs-docusaurus/organizations/managing-security-and-risk#app-scanning",children:"Enable App scanning"})," to scan Web Applications and APIs for security vulnerabilities."]}),"\n"]}),"\n",(0,o.jsx)(i.p,{children:(0,o.jsx)(i.img,{alt:"AI Risk Checklist",src:s(52186).A+"",width:"735",height:"186"})})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,o.jsx)(i,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},52186(e,i,s){s.d(i,{A:()=>n});const n=s.p+"assets/images/ai-risk-checklist-a3fa8545798a6cea98002b088da0ae4e.png"},70779(e,i,s){s.d(i,{A:()=>n});const n=s.p+"assets/images/risk-level-e6a8131a24a2ca3745c01985d9429185.png"},76876(e,i,s){s.d(i,{A:()=>n});const n=s.p+"assets/images/ai-policy-compliance-398a63f36f4f0c7c3575ab372008bc32.png"},89072(e,i,s){s.d(i,{A:()=>n});const n=s.p+"assets/images/ai-risk-hub-dashboard-56a1f99a67f442d9a76a833f4fdcc3a0.png"}}]);